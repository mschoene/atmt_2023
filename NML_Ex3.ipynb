{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mschoene/atmt_2023/blob/main/NML_Ex3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QqdCQLPymppr"
      },
      "source": [
        "# The code change is on git, this notebook is just for running the training, translation and comparison with sacrebleu"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch==2.0.1 numpy tqdm sacrebleu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zpkhsphvzeH9",
        "outputId": "c1e70959-4e66-45cc-a7b2-743750fb160b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch==2.0.1\n",
            "  Downloading torch-2.0.1-cp310-cp310-manylinux1_x86_64.whl (619.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m619.9/619.9 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.23.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.1)\n",
            "Collecting sacrebleu\n",
            "  Downloading sacrebleu-2.3.2-py3-none-any.whl (119 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.7/119.7 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (3.12.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (3.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (3.1.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch==2.0.1)\n",
            "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m84.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu11==11.7.99 (from torch==2.0.1)\n",
            "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m76.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu11==11.7.101 (from torch==2.0.1)\n",
            "  Downloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m76.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu11==8.5.0.96 (from torch==2.0.1)\n",
            "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu11==11.10.3.66 (from torch==2.0.1)\n",
            "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu11==10.9.0.58 (from torch==2.0.1)\n",
            "  Downloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu11==10.2.10.91 (from torch==2.0.1)\n",
            "  Downloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu11==11.4.0.1 (from torch==2.0.1)\n",
            "  Downloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.6/102.6 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu11==11.7.4.91 (from torch==2.0.1)\n",
            "  Downloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.2/173.2 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu11==2.14.3 (from torch==2.0.1)\n",
            "  Downloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.1/177.1 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu11==11.7.91 (from torch==2.0.1)\n",
            "  Downloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.6/98.6 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting triton==2.0.0 (from torch==2.0.1)\n",
            "  Downloading triton-2.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.3/63.3 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1) (67.7.2)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1) (0.41.2)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1) (3.27.7)\n",
            "Collecting lit (from triton==2.0.0->torch==2.0.1)\n",
            "  Downloading lit-17.0.4.tar.gz (153 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.1/153.1 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting portalocker (from sacrebleu)\n",
            "  Downloading portalocker-2.8.2-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (2023.6.3)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (0.9.0)\n",
            "Collecting colorama (from sacrebleu)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (4.9.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.0.1) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.0.1) (1.3.0)\n",
            "Building wheels for collected packages: lit\n",
            "  Building wheel for lit (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lit: filename=lit-17.0.4-py3-none-any.whl size=93257 sha256=439924b8883e5de87f8361c4269a8ecaecebbbfdf6276d1a8b3a5f7bc03ad962\n",
            "  Stored in directory: /root/.cache/pip/wheels/be/ae/00/696c57d438bfc7c0e89c4c379083ea08b1c2e54d85a5f7cd7c\n",
            "Successfully built lit\n",
            "Installing collected packages: lit, portalocker, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, colorama, sacrebleu, nvidia-cusolver-cu11, nvidia-cudnn-cu11, triton, torch\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 2.1.0\n",
            "    Uninstalling triton-2.1.0:\n",
            "      Successfully uninstalled triton-2.1.0\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.1.0+cu118\n",
            "    Uninstalling torch-2.1.0+cu118:\n",
            "      Successfully uninstalled torch-2.1.0+cu118\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.1.0+cu118 requires torch==2.1.0, but you have torch 2.0.1 which is incompatible.\n",
            "torchdata 0.7.0 requires torch==2.1.0, but you have torch 2.0.1 which is incompatible.\n",
            "torchtext 0.16.0 requires torch==2.1.0, but you have torch 2.0.1 which is incompatible.\n",
            "torchvision 0.16.0+cu118 requires torch==2.1.0, but you have torch 2.0.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed colorama-0.4.6 lit-17.0.4 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-cupti-cu11-11.7.101 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.2.10.91 nvidia-cusolver-cu11-11.4.0.1 nvidia-cusparse-cu11-11.7.4.91 nvidia-nccl-cu11-2.14.3 nvidia-nvtx-cu11-11.7.91 portalocker-2.8.2 sacrebleu-2.3.2 torch-2.0.1 triton-2.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bGKI2JQimoZI",
        "outputId": "b1d68d37-8566-4854-b6c0-10d3d89dbb54"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'atmt_2023'...\n",
            "remote: Enumerating objects: 281, done.\u001b[K\n",
            "remote: Counting objects: 100% (116/116), done.\u001b[K\n",
            "remote: Compressing objects: 100% (81/81), done.\u001b[K\n",
            "remote: Total 281 (delta 43), reused 35 (delta 35), pack-reused 165\u001b[K\n",
            "Receiving objects: 100% (281/281), 62.49 MiB | 16.15 MiB/s, done.\n",
            "Resolving deltas: 100% (69/69), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/mschoene/atmt_2023.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oafUBspAqAYR",
        "outputId": "57417a88-75a2-452c-8950-833adfcb5155"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/atmt_2023\n"
          ]
        }
      ],
      "source": [
        "%cd atmt_2023"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train with hyperparamter change - deeper network"
      ],
      "metadata": {
        "id": "2cOTNVsWPSGK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir assignments/03/checkpoints_hp_change/"
      ],
      "metadata": {
        "id": "FcKLHvEWTeIV"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "pSpttEESn46X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8688741-b4d1-42d0-f05a-956537563f11"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO: Commencing training!\n",
            "INFO: COMMAND: train.py --data data/en-fr/prepared --source-lang fr --target-lang en --save-dir assignments/03/checkpoints_hp_change --encoder-num-layers 3 --decoder-num-layers 3 --cuda --batch-size 512\n",
            "INFO: Arguments: {'cuda': True, 'data': 'data/en-fr/prepared', 'source_lang': 'fr', 'target_lang': 'en', 'max_tokens': None, 'batch_size': 512, 'train_on_tiny': False, 'arch': 'lstm', 'max_epoch': 10000, 'clip_norm': 4.0, 'lr': 0.0003, 'patience': 3, 'log_file': None, 'save_dir': 'assignments/03/checkpoints_hp_change', 'restore_file': 'checkpoint_last.pt', 'save_interval': 1, 'no_save': False, 'epoch_checkpoints': False, 'encoder_num_layers': 3, 'decoder_num_layers': 3, 'encoder_embed_dim': 64, 'encoder_embed_path': None, 'encoder_hidden_size': 64, 'encoder_bidirectional': 'True', 'encoder_dropout_in': 0.25, 'encoder_dropout_out': 0.25, 'decoder_embed_dim': 64, 'decoder_embed_path': None, 'decoder_hidden_size': 128, 'decoder_dropout_in': 0.25, 'decoder_dropout_out': 0.25, 'decoder_use_attention': 'True', 'decoder_use_lexical_model': 'False', 'device_id': 0}\n",
            "INFO: Loaded a source dictionary (fr) with 4000 words\n",
            "INFO: Loaded a target dictionary (en) with 4000 words\n",
            "INFO: Built a model with 1771424 parameters\n",
            "INFO: Loaded checkpoint assignments/03/checkpoints_hp_change/checkpoint_last.pt\n",
            "INFO: Epoch 256: loss 3.17 | lr 0.0003 | num_tokens 9.435 | batch_size 500 | grad_norm 3.852 | clip 0.3\n",
            "INFO: Epoch 256: valid_loss 3.3 | num_tokens 9.14 | batch_size 500 | valid_perplexity 27.1\n",
            "INFO: Epoch 257: loss 3.172 | lr 0.0003 | num_tokens 9.435 | batch_size 500 | grad_norm 4.26 | clip 0.4\n",
            "INFO: Epoch 257: valid_loss 3.28 | num_tokens 9.14 | batch_size 500 | valid_perplexity 26.7\n",
            "INFO: Epoch 258: loss 3.154 | lr 0.0003 | num_tokens 9.435 | batch_size 500 | grad_norm 3.736 | clip 0.4\n",
            "INFO: Epoch 258: valid_loss 3.31 | num_tokens 9.14 | batch_size 500 | valid_perplexity 27.3\n",
            "INFO: Epoch 259: loss 3.177 | lr 0.0003 | num_tokens 9.435 | batch_size 500 | grad_norm 4.576 | clip 0.6\n",
            "INFO: Epoch 259: valid_loss 3.3 | num_tokens 9.14 | batch_size 500 | valid_perplexity 27.1\n",
            "INFO: Epoch 260: loss 3.151 | lr 0.0003 | num_tokens 9.435 | batch_size 500 | grad_norm 3.744 | clip 0.2\n",
            "INFO: Epoch 260: valid_loss 3.27 | num_tokens 9.14 | batch_size 500 | valid_perplexity 26.3\n",
            "INFO: Epoch 261: loss 3.145 | lr 0.0003 | num_tokens 9.435 | batch_size 500 | grad_norm 3.267 | clip 0.1\n",
            "INFO: Epoch 261: valid_loss 3.28 | num_tokens 9.14 | batch_size 500 | valid_perplexity 26.5\n",
            "INFO: Epoch 262: loss 3.138 | lr 0.0003 | num_tokens 9.435 | batch_size 500 | grad_norm 3.646 | clip 0.3\n",
            "INFO: Epoch 262: valid_loss 3.26 | num_tokens 9.14 | batch_size 500 | valid_perplexity 26.1\n",
            "INFO: Epoch 263: loss 3.143 | lr 0.0003 | num_tokens 9.435 | batch_size 500 | grad_norm 3.328 | clip 0.15\n",
            "INFO: Epoch 263: valid_loss 3.28 | num_tokens 9.14 | batch_size 500 | valid_perplexity 26.7\n",
            "INFO: Epoch 264: loss 3.128 | lr 0.0003 | num_tokens 9.435 | batch_size 500 | grad_norm 3.523 | clip 0.2\n",
            "INFO: Epoch 264: valid_loss 3.26 | num_tokens 9.14 | batch_size 500 | valid_perplexity 26.1\n",
            "INFO: Epoch 265: loss 3.131 | lr 0.0003 | num_tokens 9.435 | batch_size 500 | grad_norm 3.427 | clip 0.15\n",
            "INFO: Epoch 265: valid_loss 3.26 | num_tokens 9.14 | batch_size 500 | valid_perplexity 26.1\n",
            "INFO: Epoch 266: loss 3.117 | lr 0.0003 | num_tokens 9.435 | batch_size 500 | grad_norm 3.405 | clip 0.2\n",
            "INFO: Epoch 266: valid_loss 3.25 | num_tokens 9.14 | batch_size 500 | valid_perplexity 25.9\n",
            "INFO: Epoch 267: loss 3.119 | lr 0.0003 | num_tokens 9.435 | batch_size 500 | grad_norm 3.307 | clip 0.1\n",
            "INFO: Epoch 267: valid_loss 3.26 | num_tokens 9.14 | batch_size 500 | valid_perplexity 26\n",
            "INFO: Epoch 268: loss 3.108 | lr 0.0003 | num_tokens 9.435 | batch_size 500 | grad_norm 3.549 | clip 0.2\n",
            "INFO: Epoch 268: valid_loss 3.25 | num_tokens 9.14 | batch_size 500 | valid_perplexity 25.8\n",
            "INFO: Epoch 269: loss 3.108 | lr 0.0003 | num_tokens 9.435 | batch_size 500 | grad_norm 3.478 | clip 0.15\n",
            "INFO: Epoch 269: valid_loss 3.25 | num_tokens 9.14 | batch_size 500 | valid_perplexity 25.8\n",
            "INFO: Epoch 270: loss 3.096 | lr 0.0003 | num_tokens 9.435 | batch_size 500 | grad_norm 3.662 | clip 0.2\n",
            "INFO: Epoch 270: valid_loss 3.25 | num_tokens 9.14 | batch_size 500 | valid_perplexity 25.7\n",
            "INFO: Epoch 271: loss 3.103 | lr 0.0003 | num_tokens 9.435 | batch_size 500 | grad_norm 3.485 | clip 0.25\n",
            "INFO: Epoch 271: valid_loss 3.25 | num_tokens 9.14 | batch_size 500 | valid_perplexity 25.9\n",
            "INFO: Epoch 272: loss 3.092 | lr 0.0003 | num_tokens 9.435 | batch_size 500 | grad_norm 3.683 | clip 0.2\n",
            "INFO: Epoch 272: valid_loss 3.24 | num_tokens 9.14 | batch_size 500 | valid_perplexity 25.6\n",
            "INFO: Epoch 273: loss 3.098 | lr 0.0003 | num_tokens 9.435 | batch_size 500 | grad_norm 3.687 | clip 0.25\n",
            "INFO: Epoch 273: valid_loss 3.25 | num_tokens 9.14 | batch_size 500 | valid_perplexity 25.7\n",
            "INFO: Epoch 274: loss 3.084 | lr 0.0003 | num_tokens 9.435 | batch_size 500 | grad_norm 3.857 | clip 0.2\n",
            "INFO: Epoch 274: valid_loss 3.24 | num_tokens 9.14 | batch_size 500 | valid_perplexity 25.4\n",
            "INFO: Epoch 275: loss 3.086 | lr 0.0003 | num_tokens 9.435 | batch_size 500 | grad_norm 3.542 | clip 0.2\n",
            "INFO: Epoch 275: valid_loss 3.24 | num_tokens 9.14 | batch_size 500 | valid_perplexity 25.6\n",
            "INFO: Epoch 276: loss 3.07 | lr 0.0003 | num_tokens 9.435 | batch_size 500 | grad_norm 3.704 | clip 0.2\n",
            "INFO: Epoch 276: valid_loss 3.23 | num_tokens 9.14 | batch_size 500 | valid_perplexity 25.2\n",
            "INFO: Epoch 277: loss 3.075 | lr 0.0003 | num_tokens 9.435 | batch_size 500 | grad_norm 3.616 | clip 0.25\n",
            "INFO: Epoch 277: valid_loss 3.23 | num_tokens 9.14 | batch_size 500 | valid_perplexity 25.3\n",
            "INFO: Epoch 278: loss 3.057 | lr 0.0003 | num_tokens 9.435 | batch_size 500 | grad_norm 3.495 | clip 0.2\n",
            "INFO: Epoch 278: valid_loss 3.22 | num_tokens 9.14 | batch_size 500 | valid_perplexity 25.1\n",
            "INFO: Epoch 279: loss 3.064 | lr 0.0003 | num_tokens 9.435 | batch_size 500 | grad_norm 3.519 | clip 0.15\n",
            "INFO: Epoch 279: valid_loss 3.22 | num_tokens 9.14 | batch_size 500 | valid_perplexity 25.1\n",
            "INFO: Epoch 280: loss 3.052 | lr 0.0003 | num_tokens 9.435 | batch_size 500 | grad_norm 3.485 | clip 0.2\n",
            "INFO: Epoch 280: valid_loss 3.21 | num_tokens 9.14 | batch_size 500 | valid_perplexity 24.9\n",
            "INFO: Epoch 281: loss 3.056 | lr 0.0003 | num_tokens 9.435 | batch_size 500 | grad_norm 3.671 | clip 0.2\n",
            "INFO: Epoch 281: valid_loss 3.22 | num_tokens 9.14 | batch_size 500 | valid_perplexity 24.9\n",
            "INFO: Epoch 282: loss 3.046 | lr 0.0003 | num_tokens 9.435 | batch_size 500 | grad_norm 3.632 | clip 0.2\n",
            "INFO: Epoch 282: valid_loss 3.21 | num_tokens 9.14 | batch_size 500 | valid_perplexity 24.7\n",
            "INFO: Epoch 283: loss 3.047 | lr 0.0003 | num_tokens 9.435 | batch_size 500 | grad_norm 3.564 | clip 0.2\n",
            "INFO: Epoch 283: valid_loss 3.21 | num_tokens 9.14 | batch_size 500 | valid_perplexity 24.7\n",
            "INFO: Epoch 284: loss 3.032 | lr 0.0003 | num_tokens 9.435 | batch_size 500 | grad_norm 3.578 | clip 0.25\n",
            "INFO: Epoch 284: valid_loss 3.2 | num_tokens 9.14 | batch_size 500 | valid_perplexity 24.6\n",
            "INFO: Epoch 285: loss 3.04 | lr 0.0003 | num_tokens 9.435 | batch_size 500 | grad_norm 3.675 | clip 0.3\n",
            "INFO: Epoch 285: valid_loss 3.21 | num_tokens 9.14 | batch_size 500 | valid_perplexity 24.8\n",
            "INFO: Epoch 286: loss 3.032 | lr 0.0003 | num_tokens 9.435 | batch_size 500 | grad_norm 3.662 | clip 0.2\n",
            "INFO: Epoch 286: valid_loss 3.2 | num_tokens 9.14 | batch_size 500 | valid_perplexity 24.5\n",
            "INFO: Epoch 287: loss 3.032 | lr 0.0003 | num_tokens 9.435 | batch_size 500 | grad_norm 3.686 | clip 0.2\n",
            "INFO: Epoch 287: valid_loss 3.2 | num_tokens 9.14 | batch_size 500 | valid_perplexity 24.5\n",
            "INFO: Epoch 288: loss 3.017 | lr 0.0003 | num_tokens 9.435 | batch_size 500 | grad_norm 4.069 | clip 0.2\n",
            "INFO: Epoch 288: valid_loss 3.19 | num_tokens 9.14 | batch_size 500 | valid_perplexity 24.3\n",
            "INFO: Epoch 289: loss 3.017 | lr 0.0003 | num_tokens 9.435 | batch_size 500 | grad_norm 3.521 | clip 0.15\n",
            "INFO: Epoch 289: valid_loss 3.2 | num_tokens 9.14 | batch_size 500 | valid_perplexity 24.5\n",
            "INFO: Epoch 290: loss 3.005 | lr 0.0003 | num_tokens 9.435 | batch_size 500 | grad_norm 3.698 | clip 0.2\n",
            "INFO: Epoch 290: valid_loss 3.19 | num_tokens 9.14 | batch_size 500 | valid_perplexity 24.2\n",
            "INFO: Epoch 291: loss 3.012 | lr 0.0003 | num_tokens 9.435 | batch_size 500 | grad_norm 3.572 | clip 0.25\n",
            "INFO: Epoch 291: valid_loss 3.19 | num_tokens 9.14 | batch_size 500 | valid_perplexity 24.2\n",
            "INFO: Epoch 292: loss 2.995 | lr 0.0003 | num_tokens 9.435 | batch_size 500 | grad_norm 3.717 | clip 0.2\n",
            "INFO: Epoch 292: valid_loss 3.18 | num_tokens 9.14 | batch_size 500 | valid_perplexity 23.9\n",
            "INFO: Epoch 293: loss 2.996 | lr 0.0003 | num_tokens 9.435 | batch_size 500 | grad_norm 3.389 | clip 0.15\n",
            "INFO: Epoch 293: valid_loss 3.18 | num_tokens 9.14 | batch_size 500 | valid_perplexity 24.1\n",
            "INFO: Epoch 294: loss 2.988 | lr 0.0003 | num_tokens 9.435 | batch_size 500 | grad_norm 3.648 | clip 0.25\n",
            "INFO: Epoch 294: valid_loss 3.17 | num_tokens 9.14 | batch_size 500 | valid_perplexity 23.8\n",
            "INFO: Epoch 295: loss 2.999 | lr 0.0003 | num_tokens 9.435 | batch_size 500 | grad_norm 3.577 | clip 0.2\n",
            "INFO: Epoch 295: valid_loss 3.17 | num_tokens 9.14 | batch_size 500 | valid_perplexity 23.9\n",
            "INFO: Epoch 296: loss 2.988 | lr 0.0003 | num_tokens 9.435 | batch_size 500 | grad_norm 3.588 | clip 0.2\n",
            "INFO: Epoch 296: valid_loss 3.17 | num_tokens 9.14 | batch_size 500 | valid_perplexity 23.7\n",
            "INFO: Epoch 297: loss 2.986 | lr 0.0003 | num_tokens 9.435 | batch_size 500 | grad_norm 3.653 | clip 0.25\n",
            "INFO: Epoch 297: valid_loss 3.17 | num_tokens 9.14 | batch_size 500 | valid_perplexity 23.7\n",
            "INFO: Epoch 298: loss 2.977 | lr 0.0003 | num_tokens 9.435 | batch_size 500 | grad_norm 3.694 | clip 0.2\n",
            "INFO: Epoch 298: valid_loss 3.16 | num_tokens 9.14 | batch_size 500 | valid_perplexity 23.6\n",
            "INFO: Epoch 299: loss 2.978 | lr 0.0003 | num_tokens 9.435 | batch_size 500 | grad_norm 3.795 | clip 0.2\n",
            "INFO: Epoch 299: valid_loss 3.16 | num_tokens 9.14 | batch_size 500 | valid_perplexity 23.6\n",
            "INFO: Epoch 300: loss 2.966 | lr 0.0003 | num_tokens 9.435 | batch_size 500 | grad_norm 3.739 | clip 0.2\n",
            "INFO: Epoch 300: valid_loss 3.16 | num_tokens 9.14 | batch_size 500 | valid_perplexity 23.5\n",
            "INFO: Epoch 301: loss 2.973 | lr 0.0003 | num_tokens 9.435 | batch_size 500 | grad_norm 3.695 | clip 0.3\n",
            "INFO: Epoch 301: valid_loss 3.16 | num_tokens 9.14 | batch_size 500 | valid_perplexity 23.6\n",
            "INFO: Epoch 302: loss 2.959 | lr 0.0003 | num_tokens 9.435 | batch_size 500 | grad_norm 3.794 | clip 0.25\n",
            "INFO: Epoch 302: valid_loss 3.14 | num_tokens 9.14 | batch_size 500 | valid_perplexity 23.2\n",
            "INFO: Epoch 303: loss 2.958 | lr 0.0003 | num_tokens 9.435 | batch_size 500 | grad_norm 3.355 | clip 0.15\n",
            "INFO: Epoch 303: valid_loss 3.15 | num_tokens 9.14 | batch_size 500 | valid_perplexity 23.3\n",
            "INFO: Epoch 304: loss 2.948 | lr 0.0003 | num_tokens 9.435 | batch_size 500 | grad_norm 3.441 | clip 0.2\n",
            "INFO: Epoch 304: valid_loss 3.14 | num_tokens 9.14 | batch_size 500 | valid_perplexity 23\n",
            "INFO: Epoch 305: loss 2.944 | lr 0.0003 | num_tokens 9.435 | batch_size 500 | grad_norm 3.341 | clip 0.1\n",
            "INFO: Epoch 305: valid_loss 3.14 | num_tokens 9.14 | batch_size 500 | valid_perplexity 23.2\n",
            "INFO: Epoch 306: loss 2.936 | lr 0.0003 | num_tokens 9.435 | batch_size 500 | grad_norm 3.533 | clip 0.2\n",
            "INFO: Epoch 306: valid_loss 3.13 | num_tokens 9.14 | batch_size 500 | valid_perplexity 22.9\n",
            "INFO: Epoch 307: loss 2.936 | lr 0.0003 | num_tokens 9.435 | batch_size 500 | grad_norm 3.377 | clip 0.1\n",
            "INFO: Epoch 307: valid_loss 3.13 | num_tokens 9.14 | batch_size 500 | valid_perplexity 23\n",
            "INFO: Epoch 308: loss 2.931 | lr 0.0003 | num_tokens 9.435 | batch_size 500 | grad_norm 3.636 | clip 0.2\n",
            "INFO: Epoch 308: valid_loss 3.13 | num_tokens 9.14 | batch_size 500 | valid_perplexity 22.9\n",
            "INFO: Epoch 309: loss 2.93 | lr 0.0003 | num_tokens 9.435 | batch_size 500 | grad_norm 3.507 | clip 0.2\n",
            "INFO: Epoch 309: valid_loss 3.13 | num_tokens 9.14 | batch_size 500 | valid_perplexity 22.8\n",
            "INFO: Epoch 310: loss 2.922 | lr 0.0003 | num_tokens 9.435 | batch_size 500 | grad_norm 3.679 | clip 0.2\n",
            "INFO: Epoch 310: valid_loss 3.13 | num_tokens 9.14 | batch_size 500 | valid_perplexity 22.8\n",
            "INFO: Epoch 311: loss 2.925 | lr 0.0003 | num_tokens 9.435 | batch_size 500 | grad_norm 3.699 | clip 0.2\n",
            "INFO: Epoch 311: valid_loss 3.13 | num_tokens 9.14 | batch_size 500 | valid_perplexity 22.8\n",
            "INFO: Epoch 312: loss 2.911 | lr 0.0003 | num_tokens 9.435 | batch_size 500 | grad_norm 3.577 | clip 0.2\n",
            "INFO: Epoch 312: valid_loss 3.12 | num_tokens 9.14 | batch_size 500 | valid_perplexity 22.6\n",
            "INFO: Epoch 313: loss 2.915 | lr 0.0003 | num_tokens 9.435 | batch_size 500 | grad_norm 3.514 | clip 0.2\n",
            "INFO: Epoch 313: valid_loss 3.12 | num_tokens 9.14 | batch_size 500 | valid_perplexity 22.6\n",
            "INFO: Epoch 314: loss 2.902 | lr 0.0003 | num_tokens 9.435 | batch_size 500 | grad_norm 3.394 | clip 0.2\n",
            "INFO: Epoch 314: valid_loss 3.11 | num_tokens 9.14 | batch_size 500 | valid_perplexity 22.5\n",
            "INFO: Epoch 315: loss 2.903 | lr 0.0003 | num_tokens 9.435 | batch_size 500 | grad_norm 3.419 | clip 0.1\n",
            "INFO: Epoch 315: valid_loss 3.11 | num_tokens 9.14 | batch_size 500 | valid_perplexity 22.4\n",
            "INFO: Epoch 316: loss 2.894 | lr 0.0003 | num_tokens 9.435 | batch_size 500 | grad_norm 3.564 | clip 0.25\n",
            "INFO: Epoch 316: valid_loss 3.1 | num_tokens 9.14 | batch_size 500 | valid_perplexity 22.2\n",
            "INFO: Epoch 317: loss 2.884 | lr 0.0003 | num_tokens 9.435 | batch_size 500 | grad_norm 3.426 | clip 0.15\n",
            "INFO: Epoch 317: valid_loss 3.1 | num_tokens 9.14 | batch_size 500 | valid_perplexity 22.2\n",
            "INFO: Epoch 318: loss 2.881 | lr 0.0003 | num_tokens 9.435 | batch_size 500 | grad_norm 3.449 | clip 0.1\n",
            "INFO: Epoch 318: valid_loss 3.1 | num_tokens 9.14 | batch_size 500 | valid_perplexity 22.1\n",
            "INFO: Epoch 319: loss 2.881 | lr 0.0003 | num_tokens 9.435 | batch_size 500 | grad_norm 3.313 | clip 0.15\n",
            "INFO: Epoch 319: valid_loss 3.09 | num_tokens 9.14 | batch_size 500 | valid_perplexity 22\n",
            "INFO: Epoch 320: loss 2.875 | lr 0.0003 | num_tokens 9.435 | batch_size 500 | grad_norm 3.5 | clip 0.1\n",
            "INFO: Epoch 320: valid_loss 3.09 | num_tokens 9.14 | batch_size 500 | valid_perplexity 21.9\n",
            "INFO: Epoch 321: loss 2.87 | lr 0.0003 | num_tokens 9.435 | batch_size 500 | grad_norm 3.264 | clip 0.1\n",
            "INFO: Epoch 321: valid_loss 3.09 | num_tokens 9.14 | batch_size 500 | valid_perplexity 22\n",
            "INFO: Epoch 322: loss 2.862 | lr 0.0003 | num_tokens 9.435 | batch_size 500 | grad_norm 3.739 | clip 0.2\n",
            "INFO: Epoch 322: valid_loss 3.08 | num_tokens 9.14 | batch_size 500 | valid_perplexity 21.9\n",
            "INFO: Epoch 323: loss 2.863 | lr 0.0003 | num_tokens 9.435 | batch_size 500 | grad_norm 3.419 | clip 0.1\n",
            "INFO: Epoch 323: valid_loss 3.09 | num_tokens 9.14 | batch_size 500 | valid_perplexity 22\n",
            "INFO: Epoch 324: loss 2.856 | lr 0.0003 | num_tokens 9.435 | batch_size 500 | grad_norm 3.576 | clip 0.2\n",
            "INFO: Epoch 324: valid_loss 3.09 | num_tokens 9.14 | batch_size 500 | valid_perplexity 22\n",
            "INFO: Epoch 325: loss 2.866 | lr 0.0003 | num_tokens 9.435 | batch_size 500 | grad_norm 3.864 | clip 0.3\n",
            "INFO: Epoch 325: valid_loss 3.09 | num_tokens 9.14 | batch_size 500 | valid_perplexity 21.9\n",
            "INFO: No validation set improvements observed for 3 epochs. Early stop!\n"
          ]
        }
      ],
      "source": [
        "!python train.py \\\n",
        "    --data data/en-fr/prepared --source-lang fr  --target-lang en --save-dir assignments/03/checkpoints_hp_change \\\n",
        "    --encoder-num-layers 3 --decoder-num-layers 3 \\\n",
        "    --cuda --batch-size 512\n",
        "\n",
        "# --decoder-dropout-in', type=float, help='dropout probability for decoder input embedding')\n",
        "# --decoder-dropout-out', type=float, help='dropout probability for decoder output')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HGII-e4EoTD1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1cf7331a-b9d4-4db3-f3b9-67dd46e282ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2023-11-07 12:34:51] COMMAND: translate.py --data data/en-fr/prepared --dicts data/en-fr/prepared --checkpoint-path assignments/03/checkpoints_hp_change/checkpoint_best.pt --output assignments/03/checkpoints_hp_change/translations.txt\n",
            "[2023-11-07 12:34:51] Arguments: {'cuda': False, 'data': 'data/en-fr/prepared', 'source_lang': 'fr', 'target_lang': 'en', 'max_tokens': None, 'batch_size': 1, 'train_on_tiny': False, 'arch': 'lstm', 'max_epoch': 10000, 'clip_norm': 4.0, 'lr': 0.0003, 'patience': 3, 'log_file': None, 'save_dir': 'assignments/03/checkpoints_hp_change', 'restore_file': 'checkpoint_last.pt', 'save_interval': 1, 'no_save': False, 'epoch_checkpoints': False, 'encoder_num_layers': 3, 'decoder_num_layers': 3, 'encoder_embed_dim': 64, 'encoder_embed_path': None, 'encoder_hidden_size': 64, 'encoder_bidirectional': 'True', 'encoder_dropout_in': 0.25, 'encoder_dropout_out': 0.25, 'decoder_embed_dim': 64, 'decoder_embed_path': None, 'decoder_hidden_size': 128, 'decoder_dropout_in': 0.25, 'decoder_dropout_out': 0.25, 'decoder_use_attention': 'True', 'decoder_use_lexical_model': 'False', 'device_id': 0, 'seed': 42, 'dicts': 'data/en-fr/prepared', 'checkpoint_path': 'assignments/03/checkpoints_hp_change/checkpoint_best.pt', 'output': 'assignments/03/checkpoints_hp_change/translations.txt', 'max_len': 128}\n",
            "[2023-11-07 12:34:51] Loaded a source dictionary (fr) with 4000 words\n",
            "[2023-11-07 12:34:51] Loaded a target dictionary (en) with 4000 words\n",
            "[2023-11-07 12:34:51] Loaded a model from checkpoint assignments/03/checkpoints_hp_change/checkpoint_best.pt\n",
            "| Generation:   4% 22/500 [01:46<40:23,  5.07s/it]"
          ]
        }
      ],
      "source": [
        "!python translate.py \\\n",
        "    --data data/en-fr/prepared \\\n",
        "    --dicts data/en-fr/prepared \\\n",
        "    --checkpoint-path assignments/03/checkpoints_hp_change/checkpoint_best.pt \\\n",
        "    --output assignments/03/checkpoints_hp_change/translations.txt \\\n",
        " #   --batch-size 64"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!bash scripts/postprocess.sh assignments/03/checkpoints_hp_change/translations.txt assignments/03/checkpoints_hp_change/translations_postProc.txt en"
      ],
      "metadata": {
        "id": "4GjH1-y8DQQ5"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cat data/en-fr/raw/test.en | sacrebleu assignments/03/checkpoints_hp_change/translations_postProc.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ccd1Yiwuo8AA",
        "outputId": "c09d5124-d738-4f02-c5cc-b2691298f9d4"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            " \"name\": \"BLEU\",\n",
            " \"score\": 3.2,\n",
            " \"signature\": \"nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.2\",\n",
            " \"verbose_score\": \"35.1/6.0/1.8/0.3 (BP = 0.949 ratio = 0.950 hyp_len = 3892 ref_len = 4095)\",\n",
            " \"nrefs\": \"1\",\n",
            " \"case\": \"mixed\",\n",
            " \"eff\": \"no\",\n",
            " \"tok\": \"13a\",\n",
            " \"smooth\": \"exp\",\n",
            " \"version\": \"2.3.2\"\n",
            "}\n",
            "\u001b[0m"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNAbAKiLOMfZnwgkPivPmNI",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}